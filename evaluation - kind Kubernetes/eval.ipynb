{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ROUGE-L"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“Š Average ROUGE-L Score for rag_v1_output: 0.5830\n",
      "ðŸ“Š Average ROUGE-L Score for rag_v2_output: 0.5783\n",
      "ðŸ“Š Average ROUGE-L Score for rag_v3_output: 0.5716\n"
     ]
    }
   ],
   "source": [
    "from rouge_score import rouge_scorer\n",
    "import pandas as pd\n",
    "\n",
    "def calculate_rouge_l(expected, generated):\n",
    "    scorer = rouge_scorer.RougeScorer(['rougeL'], use_stemmer=True)\n",
    "    scores = scorer.score(expected, generated)\n",
    "    return scores[\"rougeL\"].fmeasure  # F1-score of ROUGE-L\n",
    "\n",
    "def evaluate_rouge(csv_file):\n",
    "    df = pd.read_csv(csv_file)\n",
    "\n",
    "    # Columns for all three versions\n",
    "    versions = [\"rag_v1_output\", \"rag_v2_output\", \"rag_v3_output\"]\n",
    "    \n",
    "    # Compute ROUGE-L for each version\n",
    "    avg_rouge_scores = {}\n",
    "    for version in versions:\n",
    "        df[f\"rouge_l_{version}\"] = df.apply(lambda row: calculate_rouge_l(row[\"expected_output\"], row[version]), axis=1)\n",
    "        avg_rouge_scores[version] = df[f\"rouge_l_{version}\"].mean()\n",
    "\n",
    "    # Print results\n",
    "    for version, score in avg_rouge_scores.items():\n",
    "        print(f\"ðŸ“Š Average ROUGE-L Score for {version}: {score:.4f}\")\n",
    "\n",
    "evaluate_rouge(\"test_queries.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cosine Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“Š Average Cosine Similarity for rag_v1_output: 0.4180\n",
      "ðŸ“Š Average Cosine Similarity for rag_v2_output: 0.4308\n",
      "ðŸ“Š Average Cosine Similarity for rag_v3_output: 0.4359\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import pandas as pd\n",
    "\n",
    "def calculate_cosine_similarity(expected, generated):\n",
    "    vectorizer = TfidfVectorizer().fit([expected, generated])\n",
    "    vectors = vectorizer.transform([expected, generated])\n",
    "    return cosine_similarity(vectors)[0, 1]  # Similarity score between 0 and 1\n",
    "\n",
    "def evaluate_cosine_similarity(csv_file):\n",
    "    df = pd.read_csv(csv_file)\n",
    "\n",
    "    # Columns for all three versions\n",
    "    versions = [\"rag_v1_output\", \"rag_v2_output\", \"rag_v3_output\"]\n",
    "\n",
    "    # Compute Cosine Similarity for each version\n",
    "    avg_cosine_scores = {}\n",
    "    for version in versions:\n",
    "        df[f\"cosine_similarity_{version}\"] = df.apply(lambda row: calculate_cosine_similarity(row[\"expected_output\"], row[version]), axis=1)\n",
    "        avg_cosine_scores[version] = df[f\"cosine_similarity_{version}\"].mean()\n",
    "\n",
    "    # Print results\n",
    "    for version, score in avg_cosine_scores.items():\n",
    "        print(f\"ðŸ“Š Average Cosine Similarity for {version}: {score:.4f}\")\n",
    "\n",
    "evaluate_cosine_similarity(\"test_queries.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BLEU Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“Š Average BLEU Score for rag_v1_output: 0.0127\n",
      "ðŸ“Š Average BLEU Score for rag_v2_output: 0.0197\n",
      "ðŸ“Š Average BLEU Score for rag_v3_output: 0.0141\n"
     ]
    }
   ],
   "source": [
    "import pandas  as pd\n",
    "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
    "\n",
    "def calculate_bleu_score(expected, generated):\n",
    "    smoothing = SmoothingFunction().method1  # Handles zero n-gram overlap issues\n",
    "    return sentence_bleu([expected.split()], generated.split(), smoothing_function=smoothing)\n",
    "\n",
    "def evaluate_bleu(csv_file):\n",
    "    df = pd.read_csv(csv_file)\n",
    "\n",
    "    # Columns for all three versions\n",
    "    versions = [\"rag_v1_output\", \"rag_v2_output\", \"rag_v3_output\"]\n",
    "\n",
    "\n",
    "    # Compute BLEU Score for each version\n",
    "    avg_bleu_scores = {}\n",
    "    for version in versions:\n",
    "        df[f\"bleu_score_{version}\"] = df.apply(lambda row: calculate_bleu_score(row[\"expected_output\"], row[version]), axis=1)\n",
    "        avg_bleu_scores[version] = df[f\"bleu_score_{version}\"].mean()\n",
    "\n",
    "    # Print results\n",
    "    for version, score in avg_bleu_scores.items():\n",
    "        print(f\"ðŸ“Š Average BLEU Score for {version}: {score:.4f}\")\n",
    "\n",
    "evaluate_bleu(\"test_queries.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
