{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setting up Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "os.environ[\"LANGCHAIN_API_KEY\"] = os.getenv(\"LANGCHAIN_API_KEY\")\n",
    "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
    "os.environ[\"USER_AGENT\"] = \"myagent\"\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "os.environ[\"GROQ_API_KEY\"] = os.getenv(\"GROQ_API_KEY\")\n",
    "\n",
    "from langchain_groq import ChatGroq\n",
    "\n",
    "llm = ChatGroq(\n",
    "    model=\"llama-3.1-8b-instant\",\n",
    "    temperature=0.3,  # Reduce randomness\n",
    "    max_tokens=512  # Limit response length\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading Documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders.csv_loader import CSVLoader\n",
    "\n",
    "csv_loader = CSVLoader(file_path=\"../dataset/sample_promql_queries.csv\")\n",
    "\n",
    "csv_documents = csv_loader.load()\n",
    "\n",
    "for doc in csv_documents:\n",
    "    doc.metadata[\"source\"] = \"csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders.csv_loader import CSVLoader\n",
    "\n",
    "metric_csv_loader = CSVLoader(file_path=\"../dataset/metric_name.csv\")\n",
    "\n",
    "metric_csv_documents = metric_csv_loader.load()\n",
    "\n",
    "for doc in metric_csv_documents:\n",
    "    doc.metadata[\"source\"] = \"metric\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import bs4\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "\n",
    "urls = [\n",
    "    \"https://prometheus.io/docs/prometheus/latest/querying/basics/\",\n",
    "    \"https://prometheus.io/docs/prometheus/latest/querying/operators/\",\n",
    "    \"https://prometheus.io/docs/prometheus/latest/querying/functions/\",\n",
    "    \"https://prometheus.io/docs/prometheus/latest/querying/examples/\",\n",
    "    \"https://prometheus.io/docs/prometheus/latest/querying/api/\",\n",
    "    \"https://prometheus.io/docs/prometheus/latest/http_sd/\",\n",
    "    \"https://promlabs.com/promql-cheat-sheet/\",\n",
    "]\n",
    "\n",
    "bs4_strainer = bs4.SoupStrainer([\"h2\", \"h3\", \"p\", \"ul\", \"code\"])\n",
    "\n",
    "web_loader = WebBaseLoader(\n",
    "    web_paths=urls,\n",
    "    bs_kwargs={\"parse_only\": bs4_strainer}\n",
    ")\n",
    "\n",
    "web_documents = web_loader.load()\n",
    "\n",
    "for doc in web_documents:\n",
    "    doc.metadata[\"source\"] = \"documentation\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = csv_documents + web_documents + metric_csv_documents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chunking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1176\n"
     ]
    }
   ],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1000,\n",
    "    chunk_overlap=200,\n",
    "    add_start_index=True,\n",
    ")\n",
    "\n",
    "docs = text_splitter.split_documents(documents)\n",
    "\n",
    "print(len(docs))\n",
    "# print(docs[1151])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Embedding and Storing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_chroma import Chroma\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "\n",
    "hf_embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "\n",
    "vectorstore = Chroma.from_documents(documents=docs, embedding=hf_embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rank_bm25 import BM25Okapi\n",
    "\n",
    "def bm25_embedding_rerank_documents(query: str, top_n: int = 4, filter: dict = None) -> str:\n",
    "    \"\"\"\n",
    "    Retrieve documents using embedding similarity (from the vectorstore) and rerank them\n",
    "    using BM25 scores combined with embedding similarity scores.\n",
    "    \n",
    "    Parameters:\n",
    "      query: The user query.\n",
    "      top_n: Number of top documents to return.\n",
    "      filter: Optional dictionary to filter documents by metadata (e.g. {\"source\": \"csv\"}).\n",
    "      \n",
    "    Returns:\n",
    "      A string combining the page_content of the top_n reranked documents.\n",
    "    \"\"\"\n",
    "    # Retrieve top 10 docs (with scores) from the vectorstore using the provided filter\n",
    "    docs_with_scores = vectorstore.similarity_search_with_score(query, k=10, filter=filter)\n",
    "    \n",
    "    if not docs_with_scores:\n",
    "        return \"No documents found.\"\n",
    "\n",
    "    # Separate documents and distances\n",
    "    retrieved_docs = [doc for doc, _ in docs_with_scores]\n",
    "    distances = [dist for _, dist in docs_with_scores]\n",
    "\n",
    "    # Convert distance to a similarity score (higher is better)\n",
    "    embedding_sims = []\n",
    "    for d in distances:\n",
    "        # Ensure distance is positive\n",
    "        if d < 0:\n",
    "            d = abs(d)\n",
    "        similarity = 1 / (1 + d)\n",
    "        embedding_sims.append(similarity)\n",
    "\n",
    "    # Prepare corpus for BM25 (splitting on whitespace)\n",
    "    doc_texts = [doc.page_content for doc in retrieved_docs]\n",
    "    tokenized_corpus = [text.split() for text in doc_texts]\n",
    "    tokenized_query = query.split()\n",
    "\n",
    "    # Compute BM25 scores\n",
    "    bm25 = BM25Okapi(tokenized_corpus)\n",
    "    bm25_scores = bm25.get_scores(tokenized_query)\n",
    "\n",
    "    # Combine BM25 and embedding similarity scores using a weighted sum\n",
    "    combined_scores = []\n",
    "    for doc, bm25_score, emb_sim in zip(retrieved_docs, bm25_scores, embedding_sims):\n",
    "        final_score = 0.5 * bm25_score + 0.5 * emb_sim\n",
    "        combined_scores.append((doc, final_score))\n",
    "\n",
    "    # Sort documents by the combined score in descending order\n",
    "    reranked = sorted(combined_scores, key=lambda x: x[1], reverse=True)\n",
    "    # Get the top_n documents\n",
    "    top_docs = [doc for doc, _ in reranked[:top_n]]\n",
    "\n",
    "    # Return a concatenated string of the top documents' content\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in top_docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CSV Retriever Tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.tools import Tool\n",
    "\n",
    "def bm25_embedding_rerank_csv(query: str, top_n: int = 3) -> str:\n",
    "    \"\"\"\n",
    "    Retrieves and reranks documents from the CSV dataset (where metadata \"source\" == \"csv\").\n",
    "    \"\"\"\n",
    "    return bm25_embedding_rerank_documents(query, top_n, filter={\"source\": \"csv\"})\n",
    "\n",
    "csv_tool = Tool(\n",
    "    name=\"Example_Queries\",\n",
    "    func=bm25_embedding_rerank_csv,\n",
    "    description=\"Use for EXISTING QUERY EXAMPLES when user provides similar scenarios. Input: natural language question matching known patterns.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Documentation Retriever Tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bm25_embedding_rerank_docs(query: str, top_n: int = 2) -> str:\n",
    "    \"\"\"\n",
    "    Retrieves and reranks documents from the official documentation (where metadata \"source\" == \"documentation\").\n",
    "    \"\"\"\n",
    "    return bm25_embedding_rerank_documents(query, top_n, filter={\"source\": \"documentation\"})\n",
    "\n",
    "docs_tool = Tool(\n",
    "    name=\"Documentation_Reference\",\n",
    "    func=bm25_embedding_rerank_docs,\n",
    "    description=\"Use for SYNTAX/FUNCTION REFERENCES when needing operator usage, function parameters, or conceptual explanations.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Metric Name Retriever Tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bm25_embedding_rerank_metrics(query: str, top_n: int = 3) -> str:\n",
    "    \"\"\"\n",
    "    Retrieves and reranks metric names from the vectorstore \n",
    "    (where metadata \"source\" == \"metric\")\n",
    "    \"\"\"\n",
    "    return bm25_embedding_rerank_documents(query, top_n, filter={\"source\": \"metric\"}\n",
    "    )\n",
    "\n",
    "metrics_tool = Tool(\n",
    "    name=\"Metric_Resolver\",\n",
    "    func=bm25_embedding_rerank_metrics,\n",
    "    description=\"Use for IDENTIFYING RELEVANT METRIC NAMES when query refers to specific metrics or measurements.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Query Generation Tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_promql_query(query: str) -> str:\n",
    "    \"\"\"\n",
    "    Converts a natural language query into an optimized PromQL query.\n",
    "    Ensures syntactic correctness, efficiency, and adherence to PromQL best practices.\n",
    "    Returns only the refined PromQL query text.\n",
    "    \"\"\"\n",
    "    prompt = f\"\"\"\n",
    "    You are an expert in PromQL query generation. Convert the given natural language request into a valid, efficient, and optimized PromQL query.\n",
    "\n",
    "    - Ensure syntactic correctness and adherence to PromQL best practices.\n",
    "    - Use appropriate aggregations (e.g., sum, rate) and filters.\n",
    "    - Avoid inefficient operations like unnecessary subqueries.\n",
    "    - Return only the PromQL query without explanations.\n",
    "\n",
    "    Examples:\n",
    "    - Input: \"Total CPU usage in the last 5 minutes?\"\n",
    "      Output: sum(rate(cpu_usage[5m]))\n",
    "    - Input: \"What is the 95th percentile of request duration over the last 10 minutes?\"\n",
    "      Output: histogram_quantile(0.95, sum(rate(request_duration_bucket[10m])) by (le))\n",
    "    - Input: \"Average memory usage per node over the last hour?\"\n",
    "      Output: avg_over_time(node_memory_usage[1h])\n",
    "\n",
    "    Natural Language Query: \"{query}\"\n",
    "\n",
    "    PromQL Query:\n",
    "    \"\"\"\n",
    "    \n",
    "    generated_query = llm.invoke(prompt)\n",
    "    return generated_query.content.strip()\n",
    "\n",
    "\n",
    "query_generation_tool = Tool(\n",
    "    name=\"PromQL_Query_Generator\",\n",
    "    func=generate_promql_query,\n",
    "    description=\"Use to DIRECTLY CONVERT well-specified requirements to PromQL when confident, or after consulting examples/references.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Agent Prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "template = '''You are an expert PromQL engineer assisting with metric query creation. Strictly Follow these guidelines:\n",
    "1. Prefer direct generation for simple/clear requests\n",
    "2. Consult references only when uncertain about syntax or patterns or metrics name\n",
    "3. Never use more than 5 actions total\n",
    "4. Never use the same tool consecutively \n",
    "\n",
    "Available Tools:\n",
    "{tools}\n",
    "\n",
    "Response Format:\n",
    "Question: Natural language query to convert\n",
    "Thought: Your analysis of requirements and approach\n",
    "Action: Tool name (choose from [{tool_names}])\n",
    "Action Input: Input for the tool\n",
    "Observation: Tool's response\n",
    "... (repeat as needed, but never perform same action consecutively)\n",
    "Thought: I now know the final answer\n",
    "Final Answer: ONLY the final PromQL query\n",
    "\n",
    "Special Cases:\n",
    "- If query seems complete/valid after generation: STOP IMMEDIATELY\n",
    "- If references disagree with generation: PRIORITIZE references\n",
    "- If tools return irrelevant info â†’ Trust generation\n",
    "- If conflicting info: FLAG uncertainty in thought process\n",
    "\n",
    "Begin!\n",
    "\n",
    "Question: {input}\n",
    "Thought:{agent_scratchpad}'''\n",
    "\n",
    "agent_prompt = PromptTemplate.from_template(template)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents import AgentExecutor, create_react_agent\n",
    "\n",
    "tools = [csv_tool, docs_tool, query_generation_tool, metrics_tool]\n",
    "\n",
    "model = llm\n",
    "\n",
    "agent = create_react_agent(model, tools, agent_prompt)\n",
    "agent_executor = AgentExecutor(\n",
    "    agent=agent, \n",
    "    tools=tools, \n",
    "    verbose=True,\n",
    "    max_iterations=7,  # Hard stop after 7 steps\n",
    "    early_stopping_method=\"generate\",\n",
    "    handle_parsing_errors=\"Return best available answer immediately\" # Add error recovery\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mThought: The query seems to be asking for the 75th percentile of response times for the 'payment-processing' service over the last 5 minutes. This involves identifying the relevant metric name, determining the quantile function, and specifying the time range.\n",
      "\n",
      "Action: Metric_Resolver\n",
      "Action Input: response times for the 'payment-processing' service\u001b[0m\u001b[36;1m\u001b[1;3mdescription: Measures the response time for checkout operations.\n",
      "metric_name: checkout_response_time\n",
      "\n",
      "description: Measures the response time for HTTP requests.\n",
      "metric_name: http_response_time\n",
      "\n",
      "description: Measures the response time for API calls.\n",
      "metric_name: api_response_time\u001b[0m\u001b[32;1m\u001b[1;3mThought: Based on the Metric Resolver's output, it seems that there are multiple relevant metrics related to response times. However, the query specifically mentions the 'payment-processing' service, which suggests that 'checkout_response_time' might be the most relevant metric. I will proceed with this assumption.\n",
      "\n",
      "Action: Documentation_Reference\n",
      "Action Input: quantile function in PromQL\u001b[0m\u001b[33;1m\u001b[1;3mrate(\n",
      "    demo_api_request_duration_seconds_count[5m]\n",
      "  )[1h:]\n",
      ")More...For more details about PromQL, see the official PromQL documentation:BasicsOperatorsFunctionsExamples\n",
      "\n",
      "le where the label value denotes the inclusive upper bound of the bucket.\n",
      "(Float samples without such a label are silently ignored.) The other labels and\n",
      "the metric name are used to identify the buckets belonging to each classic\n",
      "histogram. The histogram metric\n",
      "type\n",
      "automatically provides time series with the _bucket suffix and the\n",
      "appropriate labels.The native histogram samples in b are treated each individually as a separate\n",
      "histogram to calculate the quantile from.As long as no naming collisions arise, b may contain a mix of classic\n",
      "and native histograms.Use the rate() function to specify the time window for the quantile\n",
      "calculation.Example: A histogram metric is called http_request_duration_seconds (and\n",
      "therefore the metric name for the buckets of a classic histogram is\n",
      "http_request_duration_seconds_bucket). To calculate the 90th percentile of request\n",
      "durations over the last 10m, use the following expression in case\u001b[0m\u001b[32;1m\u001b[1;3mThought: Based on the documentation reference, it seems that the `rate()` function is used to specify the time window for the quantile calculation. However, the query is asking for the 75th percentile over the last 5 minutes, which suggests that we should use the `quantile()` function instead.\n",
      "\n",
      "Action: Documentation_Reference\n",
      "Action Input: quantile function in PromQL\u001b[0m\u001b[33;1m\u001b[1;3mrate(\n",
      "    demo_api_request_duration_seconds_count[5m]\n",
      "  )[1h:]\n",
      ")More...For more details about PromQL, see the official PromQL documentation:BasicsOperatorsFunctionsExamples\n",
      "\n",
      "le where the label value denotes the inclusive upper bound of the bucket.\n",
      "(Float samples without such a label are silently ignored.) The other labels and\n",
      "the metric name are used to identify the buckets belonging to each classic\n",
      "histogram. The histogram metric\n",
      "type\n",
      "automatically provides time series with the _bucket suffix and the\n",
      "appropriate labels.The native histogram samples in b are treated each individually as a separate\n",
      "histogram to calculate the quantile from.As long as no naming collisions arise, b may contain a mix of classic\n",
      "and native histograms.Use the rate() function to specify the time window for the quantile\n",
      "calculation.Example: A histogram metric is called http_request_duration_seconds (and\n",
      "therefore the metric name for the buckets of a classic histogram is\n",
      "http_request_duration_seconds_bucket). To calculate the 90th percentile of request\n",
      "durations over the last 10m, use the following expression in case\u001b[0m\u001b[32;1m\u001b[1;3mThought: Thought: Based on the documentation reference, it seems that the `quantile()` function is the correct function to use for calculating the 75th percentile. However, I still need to specify the metric name and the time range.\n",
      "\n",
      "Action: PromQL_Query_Generator\n",
      "Action Input: quantile(0.75, checkout_response_time[5m])\u001b[0m\u001b[38;5;200m\u001b[1;3mhistogram_quantile(0.75, sum(rate(checkout_response_time[5m])) by (le))\u001b[0m\u001b[32;1m\u001b[1;3mThought: I now know the final answer\n",
      "\n",
      "Final Answer: histogram_quantile(0.75, sum(rate(checkout_response_time[5m])) by (le))\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input': \"Histogram quantile of 0.75 for the 'payment-processing' service response times in the last 5 minutes\",\n",
       " 'output': 'histogram_quantile(0.75, sum(rate(checkout_response_time[5m])) by (le))'}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example_query = \"Histogram quantile of 0.75 for the 'payment-processing' service response times in the last 5 minutes\"\n",
    "# example_query = \"Total database query time in the last 5 minute\"\n",
    "agent_executor.invoke({\"input\": example_query})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
