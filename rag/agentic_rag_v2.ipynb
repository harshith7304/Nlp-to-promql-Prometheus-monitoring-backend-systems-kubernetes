{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setting up Environment\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_groq import ChatGroq\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "os.environ[\"LANGCHAIN_API_KEY\"] = os.getenv(\"LANGCHAIN_API_KEY\")\n",
    "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
    "os.environ[\"USER_AGENT\"] = \"myagent\"\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "os.environ[\"GROQ_API_KEY\"] = os.getenv(\"GROQ_API_KEY\")\n",
    "\n",
    "llm = ChatGroq(\n",
    "    model=\"llama-3.3-70b-versatile\",\n",
    "    temperature=0.3,  # Reduce randomness\n",
    "    max_tokens=512  # Limit response length\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading Documents\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "from langchain_community.document_loaders import PyMuPDFLoader\n",
    "\n",
    "file_paths = [\n",
    "    \"../dataset/docs/prometheusDocs.pdf\",\n",
    "    \"../dataset/docs/promqlCheatSheet.pdf\",\n",
    "    \"../dataset/docs/prometheusBetterstack.pdf\",\n",
    "]\n",
    "\n",
    "pdf_documents = []\n",
    "\n",
    "# Loop through each file path and load the PDFs\n",
    "for file_path in file_paths:\n",
    "    loader = PyMuPDFLoader(file_path)\n",
    "    documents = loader.load()\n",
    "    pdf_documents.extend(documents)\n",
    "\n",
    "print(f\"Loaded {len(pdf_documents)} pages across all PDFs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders.csv_loader import CSVLoader\n",
    "\n",
    "metric_csv_loader = CSVLoader(file_path=\"../dataset/metric_name.csv\")\n",
    "\n",
    "metric_csv_documents = metric_csv_loader.load()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chunking\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1000,\n",
    "    chunk_overlap=200,\n",
    "    add_start_index=True,\n",
    ")\n",
    "\n",
    "pdf_chunks = text_splitter.split_documents(pdf_documents)\n",
    "\n",
    "\n",
    "metric_csv_chunks = text_splitter.split_documents(metric_csv_documents)\n",
    "\n",
    "print(len(pdf_chunks))\n",
    "print(len(metric_csv_chunks))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Embedding and Storing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "from langchain_milvus import Milvus\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "\n",
    "model_name = \"BAAI/bge-m3\"\n",
    "model_kwargs = {'device': 'cpu'}\n",
    "encode_kwargs = {'normalize_embeddings': True}\n",
    "\n",
    "embedding = HuggingFaceEmbeddings(\n",
    "    model_name=model_name,\n",
    "    model_kwargs=model_kwargs,\n",
    "    encode_kwargs=encode_kwargs\n",
    ")\n",
    "\n",
    "URI = \"./milvus_promql.db\"\n",
    "\n",
    "pdf_vectorstore = Milvus(\n",
    "    embedding_function=embedding,\n",
    "    collection_name=\"prometheus_pdf_docs\",\n",
    "    connection_args={\"uri\": URI},\n",
    "    drop_old=True,\n",
    "    auto_id=True\n",
    ")\n",
    "pdf_vectorstore.add_documents(documents=pdf_chunks)\n",
    "\n",
    "metric_csv_vectorstore = Milvus(\n",
    "    embedding_function=embedding,\n",
    "    collection_name=\"prometheus_metric_csv_docs\",\n",
    "    connection_args={\"uri\": URI},\n",
    "    drop_old=True,\n",
    "    auto_id=True\n",
    ")\n",
    "metric_csv_vectorstore.add_documents(documents=metric_csv_chunks)\n",
    "\n",
    "print(\"Finished adding documents to Milvus\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Retriever\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rank_bm25 import BM25Okapi\n",
    "\n",
    "def bm25_embedding_rerank_documents(query: str, top_n: int = 4, vectorstore: Milvus = None) -> str:\n",
    "    \"\"\"\n",
    "    Retrieve documents using embedding similarity (from the vectorstore) and rerank them\n",
    "    using BM25 scores combined with embedding similarity scores.\n",
    "\n",
    "    Parameters:\n",
    "      query: The user query.\n",
    "      top_n: Number of top documents to return.\n",
    "      vectorstore: The vectorstore to use for retrieving documents.\n",
    "\n",
    "    Returns:\n",
    "      A string combining the page_content of the top_n reranked documents.\n",
    "    \"\"\"\n",
    "    if vectorstore is None:\n",
    "        return \"No vectorstore provided.\"\n",
    "\n",
    "    # Retrieve top 10 docs (with scores) from the vectorstore\n",
    "    docs_with_scores = vectorstore.similarity_search_with_relevance_scores(query, k=10)\n",
    "\n",
    "    if not docs_with_scores:\n",
    "        return \"No documents found.\"\n",
    "\n",
    "    # Separate documents and distances\n",
    "    retrieved_docs = [doc for doc, _ in docs_with_scores]\n",
    "    embedding_sims = [score for _, score in docs_with_scores]\n",
    "\n",
    "    # Prepare corpus for BM25 (splitting on whitespace)\n",
    "    doc_texts = [doc.page_content for doc in retrieved_docs]\n",
    "    tokenized_corpus = [text.split() for text in doc_texts]\n",
    "    tokenized_query = query.split()\n",
    "\n",
    "    # Compute BM25 scores\n",
    "    bm25 = BM25Okapi(tokenized_corpus)\n",
    "    bm25_scores = bm25.get_scores(tokenized_query)\n",
    "\n",
    "    # Combine BM25 and embedding similarity scores using a weighted sum\n",
    "    combined_scores = []\n",
    "    for doc, bm25_score, emb_sim in zip(retrieved_docs, bm25_scores, embedding_sims):\n",
    "        final_score = 0.5 * bm25_score + 0.5 * emb_sim\n",
    "        combined_scores.append((doc, final_score))\n",
    "\n",
    "    # Sort documents by the combined score in descending order\n",
    "    reranked = sorted(combined_scores, key=lambda x: x[1], reverse=True)\n",
    "    # Get the top_n documents\n",
    "    top_docs = [doc for doc, _ in reranked[:top_n]]\n",
    "\n",
    "    # Return a concatenated string of the top documents' content\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in top_docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Documentation Retriever Tool\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.tools import Tool\n",
    "\n",
    "def bm25_embedding_rerank_docs(query: str, top_n: int = 2, vectorstore= pdf_vectorstore) -> str:\n",
    "    \"\"\"\n",
    "    Retrieves and reranks documents from the official documentation.\n",
    "    \"\"\"\n",
    "    return bm25_embedding_rerank_documents(query, top_n, vectorstore)\n",
    "\n",
    "\n",
    "docs_tool = Tool(\n",
    "    name=\"Documentation_Reference\",\n",
    "    func=bm25_embedding_rerank_docs,\n",
    "    description=\"Use for SYNTAX/FUNCTION REFERENCES when needing operator usage, function parameters, or conceptual explanations.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Metric Name Retriever Tool\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bm25_embedding_rerank_metrics(query: str, top_n: int = 3, vectorstore = metric_csv_vectorstore) -> str:\n",
    "    \"\"\"\n",
    "    Retrieves and reranks metric names from the vectorstore.\n",
    "    \"\"\"\n",
    "    return bm25_embedding_rerank_documents(query, top_n, vectorstore)\n",
    "\n",
    "\n",
    "metrics_tool = Tool(\n",
    "    name=\"Metric_Resolver\",\n",
    "    func=bm25_embedding_rerank_metrics,\n",
    "    description=\"Use for IDENTIFYING RELEVANT METRIC NAMES when query refers to specific metrics or measurements.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Query Generation Tool\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_promql_query(query: str) -> str:\n",
    "    \"\"\"\n",
    "    Converts a natural language query into an optimized PromQL query.\n",
    "    Ensures syntactic correctness, efficiency, and adherence to PromQL best practices.\n",
    "    Returns only the refined PromQL query text.\n",
    "    \"\"\"\n",
    "    prompt = f\"\"\"\n",
    "    You are an expert in PromQL query generation. Convert the given natural language request into a valid, efficient, and optimized PromQL query.\n",
    "\n",
    "    - Ensure syntactic correctness and adherence to PromQL best practices.\n",
    "    - Use appropriate aggregations (e.g., sum, rate) and filters.\n",
    "    - Avoid inefficient operations like unnecessary subqueries.\n",
    "    - Return only the PromQL query without explanations.\n",
    "\n",
    "    Examples:\n",
    "    - Input: \"Total CPU usage in the last 5 minutes?\"\n",
    "      Output: sum(rate(cpu_usage[5m]))\n",
    "    - Input: \"What is the 95th percentile of request duration over the last 10 minutes?\"\n",
    "      Output: histogram_quantile(0.95, sum(rate(request_duration_bucket[10m])) by (le))\n",
    "    - Input: \"Average memory usage per node over the last hour?\"\n",
    "      Output: avg_over_time(node_memory_usage[1h])\n",
    "\n",
    "    Natural Language Query: \"{query}\"\n",
    "\n",
    "    PromQL Query:\n",
    "    \"\"\"\n",
    "\n",
    "    generated_query = llm.invoke(prompt)\n",
    "    return generated_query.content.strip()\n",
    "\n",
    "\n",
    "query_generation_tool = Tool(\n",
    "    name=\"PromQL_Query_Generator\",\n",
    "    func=generate_promql_query,\n",
    "    description=\"Use to DIRECTLY CONVERT well-specified requirements to PromQL when confident, or after consulting examples/references.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Syntax Correction Tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fix_promql_syntax(query: str) -> str:\n",
    "    \"\"\"\n",
    "    Analyzes and corrects the syntax of a PromQL query.\n",
    "    If the query is syntactically correct, returns it unchanged.\n",
    "    Otherwise, fixes any syntax errors and returns the corrected query.\n",
    "    \"\"\"\n",
    "    prompt = f\"\"\"\n",
    "    You are a PromQL syntax expert. Analyze the following PromQL query:\n",
    "\n",
    "    {query}\n",
    "\n",
    "    The query might contain syntax errors, including mismatches between scalars and vectors \n",
    "    (for example, using a function that expects a scalar but receiving a vector, or vice versa),\n",
    "    or operator/function usage issues. If the query is syntactically correct, return it unchanged.\n",
    "    Otherwise, fix all syntax errors and return only the corrected query.\n",
    "\n",
    "    Examples:\n",
    "    - Input: \"sum_over_time(active_users[5m])\"\n",
    "      Output: \"sum_over_time(active_users[5m])\"\n",
    "    - Input: \"sum(active_users[5m])\"\n",
    "      Output: \"sum_over_time(active_users[5m])\"\n",
    "    \"\"\"\n",
    "    \n",
    "    generated_query = llm.invoke(prompt)\n",
    "    return generated_query.content.strip()\n",
    "\n",
    "\n",
    "promql_syntax_correction_tool = Tool(\n",
    "    name=\"PromQL_Syntax_Corrector\",\n",
    "    func=fix_promql_syntax,\n",
    "    description=\"Use this tool to correct syntax errors in PromQL queries, ensuring they are valid and optimized according to PromQL best practices.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Agent Prompt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "template = '''You are an expert PromQL engineer assisting with metric query creation. Strictly Follow these guidelines:\n",
    "1. Prefer direct generation for simple/clear requests\n",
    "2. Consult references only when uncertain about syntax or patterns or metrics name\n",
    "3. Never use more than 5 actions total\n",
    "4. Never use the same tool consecutively \n",
    "\n",
    "Available Tools:\n",
    "{tools}\n",
    "\n",
    "Response Format:\n",
    "Question: Natural language query to convert\n",
    "Thought: Your analysis of requirements and approach\n",
    "Action: Tool name (choose from [{tool_names}])\n",
    "Action Input: Input for the tool\n",
    "Observation: Tool's response\n",
    "... (repeat as needed, but never perform same action consecutively)\n",
    "Thought: I now know the final answer\n",
    "Final Answer: ONLY the final PromQL query\n",
    "\n",
    "Special Cases:\n",
    "- If query seems complete/valid after generation: STOP IMMEDIATELY\n",
    "- If references disagree with generation: PRIORITIZE references\n",
    "- If tools return irrelevant info: Trust generation\n",
    "- If conflicting info: FLAG uncertainty in thought process\n",
    "\n",
    "Begin!\n",
    "\n",
    "Question: {input}\n",
    "Thought:{agent_scratchpad}'''\n",
    "\n",
    "agent_prompt = PromptTemplate.from_template(template)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Agent\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents import AgentExecutor, create_react_agent\n",
    "\n",
    "tools = [docs_tool, metrics_tool, query_generation_tool, promql_syntax_correction_tool]\n",
    "\n",
    "model = llm\n",
    "\n",
    "agent = create_react_agent(model, tools, agent_prompt)\n",
    "agent_executor = AgentExecutor(\n",
    "    agent=agent,\n",
    "    tools=tools,\n",
    "    max_iterations=7,  # Hard stop after 7 steps\n",
    "    early_stopping_method=\"generate\",\n",
    "    handle_parsing_errors=\"Return best available answer immediately\"  # Add error recovery\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "example_query = \"Average request latency over the last 24 hours\"\n",
    "agent_executor.invoke({\"input\": example_query})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "df = pd.read_csv(\"../evaluation/test_queries.csv\")\n",
    "\n",
    "if 'agentic_rag_v2_output' not in df.columns:\n",
    "    df['agentic_rag_v2_output'] = [''] * len(df)\n",
    "\n",
    "batch_size = 5\n",
    "num_batches = (len(df) + batch_size - 1) // batch_size\n",
    "\n",
    "for batch_num in range(num_batches):\n",
    "    start_idx = batch_num * batch_size\n",
    "    end_idx = min((batch_num + 1) * batch_size, len(df))\n",
    "    batch_indices = range(start_idx, end_idx)\n",
    "\n",
    "    agentic_rag_v2_outputs = []\n",
    "\n",
    "    for idx in tqdm(batch_indices, desc=f\"Processing Batch {batch_num + 1}/{num_batches}\"):\n",
    "        if pd.notna(df.loc[idx, 'agentic_rag_v2_output']) and df.loc[idx, 'agentic_rag_v2_output'].strip():\n",
    "            agentic_rag_v2_outputs.append(df.loc[idx, 'agentic_rag_v2_output'])\n",
    "            continue\n",
    "\n",
    "        query = df.loc[idx, 'nl_query']\n",
    "        \n",
    "        try:\n",
    "            result = agent_executor.invoke({\"input\": query})\n",
    "            # Extract the 'output' field from the result dictionary.\n",
    "            agentic_rag_v2_final_output = result[\"output\"] if isinstance(result, dict) else result\n",
    "        except Exception as e:\n",
    "            agentic_rag_v2_final_output = \"ERROR\"\n",
    "\n",
    "        agentic_rag_v2_outputs.append(agentic_rag_v2_final_output)\n",
    "\n",
    "    # Update the DataFrame with new results\n",
    "    df.iloc[start_idx:end_idx, df.columns.get_loc('agentic_rag_v2_output')] = agentic_rag_v2_outputs\n",
    "    \n",
    "    # Save progress after each batch\n",
    "    df.to_csv(\"../evaluation/test_queries.csv\", index=False)\n",
    "\n",
    "print(\"Processing complete!\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
