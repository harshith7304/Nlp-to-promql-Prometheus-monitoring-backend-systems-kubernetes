{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setting up Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "os.environ[\"LANGCHAIN_API_KEY\"] = os.getenv(\"LANGCHAIN_API_KEY\")\n",
    "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
    "os.environ[\"USER_AGENT\"] = \"myagent\"\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "os.environ[\"GROQ_API_KEY\"] = os.getenv(\"GROQ_API_KEY\")\n",
    "\n",
    "from langchain_groq import ChatGroq\n",
    "\n",
    "llm = ChatGroq(model=\"llama-3.1-8b-instant\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading Documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 122 pages across all PDFs\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders import PyMuPDFLoader\n",
    "\n",
    "file_paths = [\n",
    "    \"../dataset/docs/prometheusDocs.pdf\",\n",
    "    \"../dataset/docs/promqlCheatSheet.pdf\",\n",
    "    \"../dataset/docs/prometheusBetterstack.pdf\",\n",
    "]\n",
    "\n",
    "pdf_documents = []\n",
    "\n",
    "# Loop through each file path and load the PDFs\n",
    "for file_path in file_paths:\n",
    "    loader = PyMuPDFLoader(file_path)\n",
    "    documents = loader.load()\n",
    "    pdf_documents.extend(documents)\n",
    "\n",
    "# Check the total number of documents (pages) loaded\n",
    "print(f\"Loaded {len(pdf_documents)} pages across all PDFs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders.csv_loader import CSVLoader\n",
    "\n",
    "csv_loader = CSVLoader(file_path=\"../dataset/sample_promql_queries.csv\")\n",
    "csv_documents = csv_loader.load()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chunking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "216\n",
      "907\n"
     ]
    }
   ],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1000,\n",
    "    chunk_overlap=200,\n",
    "    add_start_index=True,\n",
    ")\n",
    "\n",
    "pdf_chunks = text_splitter.split_documents(pdf_documents)\n",
    "\n",
    "csv_chunks = text_splitter.split_documents(csv_documents)\n",
    "\n",
    "print(len(pdf_chunks))\n",
    "print(len(csv_chunks))\n",
    "# print(pdf_chunks[4].page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Embedding and Storing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_milvus import Milvus\n",
    "from langchain_community.embeddings import HuggingFaceBgeEmbeddings\n",
    "\n",
    "model_name = \"BAAI/bge-m3\"\n",
    "model_kwargs = {'device': 'cpu'}\n",
    "encode_kwargs = {'normalize_embeddings': True}\n",
    "\n",
    "embedding = HuggingFaceBgeEmbeddings(\n",
    "    model_name=model_name,\n",
    "    model_kwargs=model_kwargs,\n",
    "    encode_kwargs=encode_kwargs\n",
    ")\n",
    "\n",
    "URI = \"./milvus_promql.db\"\n",
    "\n",
    "pdf_vectorstore = Milvus.from_documents(\n",
    "    documents=pdf_chunks,\n",
    "    embedding=embedding,\n",
    "    collection_name=\"prometheus_pdf_docs\",\n",
    "    connection_args={\"uri\": URI},\n",
    "    drop_old=True,\n",
    ")\n",
    "\n",
    "csv_vectorstore = Milvus.from_documents(\n",
    "    documents=csv_chunks,\n",
    "    embedding=embedding,\n",
    "    collection_name=\"prometheus_csv_docs\",\n",
    "    connection_args={\"uri\": URI},\n",
    "    drop_old=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Context Retreival"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf_retriever = pdf_vectorstore.as_retriever(search_kwargs={\"k\": 4})\n",
    "\n",
    "csv_retriever = csv_vectorstore.as_retriever(search_kwargs={\"k\": 2})\n",
    "\n",
    "# retrieved_docs = csv_retriever.invoke(\"Get the average CPU usage over the last 30 minutes.\")\n",
    "# print(retrieved_docs[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prompt Template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "template = \"\"\"\n",
    "You are an AI assistant that generates PromQL queries from natural language descriptions.\n",
    "You have access to the following context, which includes:\n",
    "- **Similar Queries**: Examples of natural language queries paired with their corresponding PromQL queries to help you understand how to translate user requests into PromQL syntax.\n",
    "- **Relevant Documentation**: Excerpts from Prometheus documentation providing detailed information about available metrics, labels, and PromQL functions to support query formulation.\n",
    "\n",
    "### Context:\n",
    "**Similar Queries:**  \n",
    "{csv_context}\n",
    "\n",
    "**Relevant Documentation:**  \n",
    "{pdf_context}\n",
    "\n",
    "### Instructions:\n",
    "1. **Understand the input**: Break down the user's natural language query and identify the key metrics, time ranges, and operations required (e.g., avg, sum, rate, histogram_quantile).\n",
    "2. **Map to Prometheus metrics**: Identify the relevant Prometheus metrics and labels that will provide the required data (e.g., cpu_usage, http_requests_total, etc.).\n",
    "3. **Formulate the query**: Using the identified metrics, generate the correct PromQL query by applying appropriate functions and operators.\n",
    "4. **Explain the logic**: Briefly describe the reasoning for the chosen metrics and PromQL functions.\n",
    "5. **Output the final query**: Provide the final PromQL query as output.\n",
    "\n",
    "### Example:\n",
    "**Input:**  \n",
    "User query: \"What is the average CPU usage over the last hour?\"\n",
    "\n",
    "**Output: **\n",
    "1. **Identify key elements**: The user is asking for the \"average CPU usage\" over \"the last hour\".\n",
    "2. **Metrics**: The metric `cpu_usage_percent` or a similar metric will be used to represent CPU usage.\n",
    "3. **Time range**: The user specified \"last hour\", so we'll apply a time range filter of `[1h]`.\n",
    "4. **PromQL Function**: We will use `avg()` to calculate the average.\n",
    "5. **Final Query**:  \n",
    "`avg(cpu_usage_percent[1h])`\n",
    "\n",
    "### Input:\n",
    "{query}\n",
    "\n",
    "### Output:\n",
    "\"\"\"\n",
    "\n",
    "custom_rag_prompt = PromptTemplate.from_template(template)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RAG Chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'### 1. Understand the input\\n\\n* **Key elements**: The user is asking for the \"total number of requests\" to the \\'/checkout\\' service over \"the last 10 minutes\".\\n* **Time range**: The user specified \"last 10 minutes\", so we\\'ll apply a time range filter of `[10m]`.\\n* **Service**: The user specified the \\'/checkout\\' service, which we\\'ll use to filter the requests.\\n\\n### 2. Map to Prometheus metrics\\n\\n* **Metric**: The `http_requests_total` metric will be used to represent the total number of requests.\\n* **Labels**: We\\'ll use the `service` label to filter the requests to the \\'/checkout\\' service.\\n\\n### 3. Formulate the query\\n\\n* **Time range**: Apply a time range filter of `[10m]` to the `http_requests_total` metric.\\n* **Filter by service**: Use the `service` label to filter the requests to the \\'/checkout\\' service.\\n* **Aggregate**: Use the `sum()` function to calculate the total number of requests.\\n\\n### 4. Explain the logic\\n\\n* We\\'re using the `sum()` function because the user asked for the \"total number of requests\".\\n* We\\'re filtering the requests by the `/checkout` service using the `service` label.\\n\\n### 5. Output the final query\\n\\n```sql\\nsum(http_requests_total{service=\"/checkout\"}[10m])\\n```'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "def format_docs(docs):\n",
    "  return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "rag_chain = (\n",
    "    {\"csv_context\": csv_retriever | format_docs ,\n",
    "     \"pdf_context\": pdf_retriever | format_docs ,\n",
    "     \"query\": RunnablePassthrough()}\n",
    "    | custom_rag_prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "rag_chain.invoke(\"Total number of requests to the '/checkout' service in the last 10 minutes\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
