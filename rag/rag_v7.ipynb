{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setting up Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "os.environ[\"LANGCHAIN_API_KEY\"] = os.getenv(\"LANGCHAIN_API_KEY\")\n",
    "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
    "os.environ[\"USER_AGENT\"] = \"myagent\"\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "os.environ[\"GROQ_API_KEY\"] = os.getenv(\"GROQ_API_KEY\")\n",
    "\n",
    "from langchain_groq import ChatGroq\n",
    "\n",
    "llm = ChatGroq(model=\"llama-3.1-8b-instant\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading Documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 122 pages across all PDFs\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders import PyMuPDFLoader\n",
    "\n",
    "file_paths = [\n",
    "    \"../dataset/docs/prometheusDocs.pdf\",\n",
    "    \"../dataset/docs/promqlCheatSheet.pdf\",\n",
    "    \"../dataset/docs/prometheusBetterstack.pdf\",\n",
    "]\n",
    "\n",
    "pdf_documents = []\n",
    "\n",
    "# Loop through each file path and load the PDFs\n",
    "for file_path in file_paths:\n",
    "    loader = PyMuPDFLoader(file_path)\n",
    "    documents = loader.load()\n",
    "    pdf_documents.extend(documents)\n",
    "\n",
    "# Check the total number of documents (pages) loaded\n",
    "print(f\"Loaded {len(pdf_documents)} pages across all PDFs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders.csv_loader import CSVLoader\n",
    "\n",
    "csv_loader = CSVLoader(file_path=\"../dataset/metric_name.csv\")\n",
    "csv_documents = csv_loader.load()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chunking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "216\n",
      "68\n"
     ]
    }
   ],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1000,\n",
    "    chunk_overlap=200,\n",
    "    add_start_index=True,\n",
    ")\n",
    "\n",
    "pdf_chunks = text_splitter.split_documents(pdf_documents)\n",
    "\n",
    "csv_chunks = text_splitter.split_documents(csv_documents)\n",
    "\n",
    "print(len(pdf_chunks))\n",
    "print(len(csv_chunks))\n",
    "# print(pdf_chunks[4].page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Embedding and Storing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_milvus import Milvus\n",
    "from langchain_community.embeddings import HuggingFaceBgeEmbeddings\n",
    "\n",
    "model_name = \"BAAI/bge-m3\"\n",
    "model_kwargs = {'device': 'cpu'}\n",
    "encode_kwargs = {'normalize_embeddings': True}\n",
    "\n",
    "embedding = HuggingFaceBgeEmbeddings(\n",
    "    model_name=model_name,\n",
    "    model_kwargs=model_kwargs,\n",
    "    encode_kwargs=encode_kwargs\n",
    ")\n",
    "\n",
    "URI = \"./milvus_promql.db\"\n",
    "\n",
    "pdf_vectorstore = Milvus.from_documents(\n",
    "    documents=pdf_chunks,\n",
    "    embedding=embedding,\n",
    "    collection_name=\"prometheus_pdf_docs\",\n",
    "    connection_args={\"uri\": URI},\n",
    "    drop_old=True,\n",
    ")\n",
    "\n",
    "csv_vectorstore = Milvus.from_documents(\n",
    "    documents=csv_chunks,\n",
    "    embedding=embedding,\n",
    "    collection_name=\"prometheus_csv_docs\",\n",
    "    connection_args={\"uri\": URI},\n",
    "    drop_old=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Context Retreival"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf_retriever = pdf_vectorstore.as_retriever(search_kwargs={\"k\": 4})\n",
    "\n",
    "csv_retriever = csv_vectorstore.as_retriever(search_kwargs={\"k\": 4})\n",
    "\n",
    "# retrieved_docs = csv_retriever.invoke(\"Give me the total number of active connections for pods where the memory usage has been above 90% for more than 15 minutes.\")\n",
    "# for doc in retrieved_docs:\n",
    "#     print(doc.page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prompt Template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "template = \"\"\"\n",
    "You are an AI assistant that generates PromQL queries from natural language descriptions.\n",
    "You have access to the following context, which includes:\n",
    "- Metric Descriptions: A list of available Prometheus metrics with their names and descriptions to help you identify the correct metrics for the user's query.\n",
    "- Relevant Documentation: Excerpts from Prometheus documentation providing detailed information about labels, PromQL functions, and syntax to support query formulation.\n",
    "\n",
    "### Context:\n",
    "Metric Descriptions:\n",
    "{csv_context}\n",
    "\n",
    "Relevant Documentation:  \n",
    "{pdf_context}\n",
    "\n",
    "### Instructions:\n",
    "1. Analyze the natural language input to identify key metrics, time ranges, and functions.\n",
    "2. Use Prometheus functions and operators where appropriate.\n",
    "3. Ensure that the syntax is correct for PromQL and provides the required information.\n",
    "4. Keep the answer concise, focusing only on the final PromQL query.\n",
    "\n",
    "### Input:\n",
    "{query}\n",
    "\n",
    "### Example:\n",
    "**Input**: \"What is the average CPU usage over the past 5 minutes?\"\n",
    "\n",
    "**Output**: `avg(rate(cpu_usage[5m]))`\n",
    "\n",
    "### Output:\n",
    "\"\"\"\n",
    "\n",
    "custom_rag_prompt = PromptTemplate.from_template(template)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RAG Chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'To calculate the total number of requests to the \\'/checkout\\' service in the last 10 minutes, we need to analyze the provided information.\\n\\nThe relevant metric for this query is `http_requests_total` with a label `path=\"/checkout\"`. However, we don\\'t have an exact metric for the \\'/checkout\\' service, so we\\'ll use the general `http_requests` metric and filter the results using the `path` label.\\n\\nHere\\'s the PromQL query:\\n```sql\\nsum(http_requests{path=\"/checkout\"}[10m])\\n```\\nThis query calculates the sum of `http_requests` over the last 10 minutes, with results filtered for the `/checkout` path.'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "def format_docs(docs):\n",
    "  return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "rag_chain = (\n",
    "    {\"csv_context\": csv_retriever | format_docs ,\n",
    "     \"pdf_context\": pdf_retriever | format_docs ,\n",
    "     \"query\": RunnablePassthrough()}\n",
    "    | custom_rag_prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "rag_chain.invoke(\"Total number of requests to the '/checkout' service in the last 10 minutes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "promql_extraction_template = \"\"\"\n",
    "From the input provided, extract and return only the final PromQL query. Do not include any additional text or formatting.\n",
    "\n",
    "input: {query}\n",
    "\"\"\"\n",
    "\n",
    "promql_extraction_prompt = PromptTemplate.from_template(promql_extraction_template)\n",
    "\n",
    "promql_extraction_chain = (\n",
    "    {\"query\": RunnablePassthrough()}\n",
    "    | promql_extraction_prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Batch 1/10: 100%|██████████| 20/20 [02:28<00:00,  7.42s/it]\n",
      "Processing Batch 2/10: 100%|██████████| 20/20 [04:37<00:00, 13.89s/it]\n",
      "Processing Batch 3/10: 100%|██████████| 20/20 [04:02<00:00, 12.14s/it]\n",
      "Processing Batch 4/10: 100%|██████████| 20/20 [03:13<00:00,  9.69s/it]\n",
      "Processing Batch 5/10: 100%|██████████| 20/20 [07:03<00:00, 21.18s/it]\n",
      "Processing Batch 6/10: 100%|██████████| 20/20 [03:52<00:00, 11.62s/it]\n",
      "Processing Batch 7/10: 100%|██████████| 20/20 [03:50<00:00, 11.54s/it]\n",
      "Processing Batch 8/10: 100%|██████████| 20/20 [03:59<00:00, 11.96s/it]\n",
      "Processing Batch 9/10: 100%|██████████| 20/20 [04:05<00:00, 12.27s/it]\n",
      "Processing Batch 10/10: 100%|██████████| 6/6 [01:26<00:00, 14.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing complete!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "df = pd.read_csv(\"../evaluation/test_queries.csv\")\n",
    "\n",
    "if 'rag_v7_output' not in df.columns:\n",
    "    df['rag_v7_output'] = [''] * len(df)\n",
    "\n",
    "batch_size = 20\n",
    "num_batches = (len(df) + batch_size - 1) // batch_size\n",
    "\n",
    "for batch_num in range(num_batches):\n",
    "    start_idx = batch_num * batch_size\n",
    "    end_idx = min((batch_num + 1) * batch_size, len(df))\n",
    "    batch_indices = range(start_idx, end_idx)\n",
    "\n",
    "    rag_v7_outputs = []\n",
    "\n",
    "    for idx in tqdm(batch_indices, desc=f\"Processing Batch {batch_num + 1}/{num_batches}\"):\n",
    "        if pd.notna(df.loc[idx, 'rag_v7_output']) and df.loc[idx, 'rag_v7_output'].strip():\n",
    "            rag_v7_outputs.append(df.loc[idx, 'rag_v7_output'])\n",
    "            continue\n",
    "\n",
    "        query = df.loc[idx, 'nl_query']\n",
    "        \n",
    "        try:\n",
    "            rag_v7_output = rag_chain.invoke(query)\n",
    "            rag_v7_final_output = promql_extraction_chain.invoke(rag_v7_output)\n",
    "        except Exception as e:\n",
    "            rag_v7_final_output = \"ERROR\"\n",
    "\n",
    "        rag_v7_outputs.append(rag_v7_final_output)\n",
    "\n",
    "    # Update the DataFrame with new results\n",
    "    df.loc[start_idx:end_idx - 1, 'rag_v7_output'] = rag_v7_outputs\n",
    "    \n",
    "    # Save progress after each batch\n",
    "    df.to_csv(\"../evaluation/test_queries.csv\", index=False)\n",
    "\n",
    "print(\"Processing complete!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
