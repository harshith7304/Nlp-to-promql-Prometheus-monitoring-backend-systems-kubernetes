{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setting up Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "os.environ[\"LANGCHAIN_API_KEY\"] = os.getenv(\"LANGCHAIN_API_KEY\")\n",
    "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
    "os.environ[\"USER_AGENT\"] = \"myagent\"\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "os.environ[\"GROQ_API_KEY\"] = os.getenv(\"GROQ_API_KEY\")\n",
    "\n",
    "from langchain_groq import ChatGroq\n",
    "\n",
    "llm = ChatGroq(model=\"llama-3.1-8b-instant\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading Documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders.csv_loader import CSVLoader\n",
    "\n",
    "csv_loader = CSVLoader(file_path=\"../dataset/sample_promql_queries.csv\")\n",
    "csv_documents = csv_loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import bs4\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "\n",
    "urls = [\n",
    "    \"https://prometheus.io/docs/prometheus/latest/querying/basics/\",\n",
    "    \"https://prometheus.io/docs/prometheus/latest/querying/operators/\",\n",
    "    \"https://prometheus.io/docs/prometheus/latest/querying/functions/\",\n",
    "    \"https://prometheus.io/docs/prometheus/latest/querying/examples/\",\n",
    "    \"https://prometheus.io/docs/prometheus/latest/querying/api/\",\n",
    "    \"https://prometheus.io/docs/prometheus/latest/http_sd/\",\n",
    "    \"https://promlabs.com/promql-cheat-sheet/\",\n",
    "]\n",
    "\n",
    "bs4_strainer = bs4.SoupStrainer([\"h2\", \"h3\", \"p\", \"ul\", \"code\"])\n",
    "\n",
    "web_loader = WebBaseLoader(\n",
    "    web_paths=urls,\n",
    "    bs_kwargs={\"parse_only\": bs4_strainer}\n",
    ")\n",
    "\n",
    "web_documents = web_loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize metadata\n",
    "for doc in web_documents:\n",
    "    doc.metadata[\"row\"] = -1\n",
    "\n",
    "# Combine documents\n",
    "documents = csv_documents + web_documents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chunking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1000,\n",
    "    chunk_overlap=200,\n",
    "    add_start_index=True,\n",
    ")\n",
    "\n",
    "docs = text_splitter.split_documents(documents)\n",
    "\n",
    "# print(len(docs))\n",
    "# print(docs[1020].page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Embedding and Storing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "370b5febc8c8467db73b52f5a644ff09",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:  32%|###1      | 724M/2.27G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from langchain_milvus import Milvus\n",
    "from langchain_community.embeddings import HuggingFaceBgeEmbeddings\n",
    "\n",
    "model_name = \"BAAI/bge-m3\"\n",
    "model_kwargs = {'device': 'cpu'}\n",
    "encode_kwargs = {'normalize_embeddings': True}\n",
    "\n",
    "embedding = HuggingFaceBgeEmbeddings(\n",
    "    model_name=model_name,\n",
    "    model_kwargs=model_kwargs,\n",
    "    encode_kwargs=encode_kwargs\n",
    ")\n",
    "\n",
    "URI = \"./milvus_promql.db\"\n",
    "\n",
    "vectorstore = Milvus.from_documents(\n",
    "    documents=docs,\n",
    "    embedding=embedding,\n",
    "    collection_name=\"prometheus_docs\",\n",
    "    connection_args={\"uri\": URI},\n",
    "    drop_old=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Context Retreival"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = vectorstore.as_retriever(search_kwargs={\"k\": 4})\n",
    "\n",
    "# retrieved_docs = retriever.invoke(\"Get the average CPU usage over the last 30 minutes.\")\n",
    "# print(retrieved_docs[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prompt Template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "template = \"\"\"\n",
    "You are an AI assistant that generates PromQL queries from natural language descriptions.\n",
    "You have access to the following context, which contains relevant information about metrics, labels, and PromQL syntax.\n",
    "\n",
    "### Context:\n",
    "{context}\n",
    "\n",
    "### Instructions:\n",
    "1. **Understand the input**: Break down the user's natural language query and identify the key metrics, time ranges, and operations required (e.g., avg, sum, rate, histogram_quantile).\n",
    "2. **Map to Prometheus metrics**: Identify the relevant Prometheus metrics and labels that will provide the required data (e.g., cpu_usage, http_requests_total, etc.).\n",
    "3. **Formulate the query**: Using the identified metrics, generate the correct PromQL query by applying appropriate functions and operators.\n",
    "4. **Explain the logic**: Briefly describe the reasoning for the chosen metrics and PromQL functions.\n",
    "5. **Output the final query**: Provide the final PromQL query as output.\n",
    "\n",
    "**Important**: Follow this structure exactly. Do not skip or alter any of the steps. The output must strictly adhere to the template, with sections clearly labeled and the PromQL query correctly formulated.\n",
    "\n",
    "### Example:\n",
    "**Input:**  \n",
    "User query: \"What is the average CPU usage over the last hour?\"\n",
    "\n",
    "**Output:**\n",
    "1. **Identify key elements**: The user is asking for the \"average CPU usage\" over \"the last hour\".\n",
    "2. **Metrics**: The metric `cpu_usage_percent` or a similar metric will be used to represent CPU usage.\n",
    "3. **Time range**: The user specified \"last hour\", so we'll apply a time range filter of `[1h]`.\n",
    "4. **PromQL Function**: We will use `avg()` to calculate the average.\n",
    "5. **Final Query**:  \n",
    "`avg(cpu_usage_percent[1h])`\n",
    "\n",
    "### Input:\n",
    "{query}\n",
    "\n",
    "### Output:\n",
    "\"\"\"\n",
    "\n",
    "custom_rag_prompt = PromptTemplate.from_template(template)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RAG Chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1. **Identify key elements**: The user is asking for the \"total number of requests\" to the \\'/checkout\\' service over \"the last 10 minutes\".\\n2. **Metrics**: The metric `http_requests_total` will be used to represent the total number of HTTP requests.\\n3. **Labels**: We need to filter for the \\'/checkout\\' service, so we\\'ll use the `service` label.\\n4. **Time range**: The user specified \"last 10 minutes\", so we\\'ll apply a time range filter of `[10m]`.\\n5. **PromQL Function**: We will use `sum()` to calculate the total and `rate()` to count the number of requests per second.\\n6. **Final Query**:  \\n`sum(rate(http_requests_total{service=\"/checkout\"}[10m]))`'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "def format_docs(docs):\n",
    "  return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "rag_chain = (\n",
    "    {\"context\": retriever | format_docs , \"query\": RunnablePassthrough()}\n",
    "    | custom_rag_prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "rag_chain.invoke(\"Total number of requests to the '/checkout' service in the last 10 minutes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Output generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "promql_extraction_template = \"\"\"\n",
    "From the input provided, extract and return only the PromQL query. Do not include any additional text or formatting.\n",
    "\n",
    "input: {query}\n",
    "\"\"\"\n",
    "\n",
    "promql_extraction_prompt = PromptTemplate.from_template(promql_extraction_template)\n",
    "\n",
    "promql_extraction_chain = (\n",
    "    {\"query\": RunnablePassthrough()}\n",
    "    | promql_extraction_prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"../evaluation/test_queries.csv\")\n",
    "\n",
    "df['rag_v2_output'] = [''] * len(df)\n",
    "\n",
    "rag_v2_outputs = [];\n",
    "\n",
    "for query in df['nl_query']:\n",
    "    rag_v2_output = rag_chain.invoke(query)\n",
    "    final_output = promql_extraction_chain.invoke(rag_v2_output)\n",
    "    rag_v2_outputs.append(final_output)\n",
    "\n",
    "df['rag_v2_output'] = rag_v2_outputs\n",
    "\n",
    "df.to_csv('../evaluation/test_queries.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
