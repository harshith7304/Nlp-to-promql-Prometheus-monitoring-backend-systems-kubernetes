{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "os.environ[\"LANGCHAIN_API_KEY\"] = os.getenv(\"LANGCHAIN_API_KEY\")\n",
    "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
    "os.environ[\"USER_AGENT\"] = \"myagent\"\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "os.environ[\"GROQ_API_KEY\"] = os.getenv(\"GROQ_API_KEY\")\n",
    "\n",
    "from langchain_groq import ChatGroq\n",
    "\n",
    "llm = ChatGroq(model=\"llama-3.3-70b-versatile\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "fix_syntax_template = \"\"\"\n",
    "You are a PromQL syntax expert. Analyze the following PromQL query:\n",
    "\n",
    "{query}\n",
    "\n",
    "The query might contain syntax errors, including mismatches between scalars and vectors \n",
    "(for example, using a function that expects a scalar but receiving a vector, or vice versa),\n",
    "or operator/function usage issues. If the query is syntactically correct, return it unchanged.\n",
    "Otherwise, fix all syntax errors and return only the corrected query.\n",
    "\n",
    "Examples:\n",
    "- Input: \"sum_over_time(active_users[5m])\"\n",
    "  Output: \"sum_over_time(active_users[5m])\"\n",
    "- Input: \"sum(active_users[5m])\"\n",
    "  Output: \"sum_over_time(active_users[5m])\"\n",
    "\n",
    "Output only the final query.\n",
    "\"\"\"\n",
    "\n",
    "fix_syntax_prompt = PromptTemplate.from_template(fix_syntax_template)\n",
    "\n",
    "fix_syntax_chain = (\n",
    "    {\"query\": RunnablePassthrough()}\n",
    "    | fix_syntax_prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'avg_over_time(db_query_response_time[5m])'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fix_syntax_chain.invoke(\"avg(db_query_response_time[5m])\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Batch 1/10: 100%|██████████| 20/20 [00:00<00:00, 30045.16it/s]\n",
      "Processing Batch 2/10: 100%|██████████| 20/20 [00:00<00:00, 30164.00it/s]\n",
      "Processing Batch 3/10: 100%|██████████| 20/20 [00:00<00:00, 38818.18it/s]\n",
      "Processing Batch 4/10: 100%|██████████| 20/20 [00:39<00:00,  2.00s/it]\n",
      "Processing Batch 5/10: 100%|██████████| 20/20 [00:46<00:00,  2.32s/it]\n",
      "Processing Batch 6/10: 100%|██████████| 20/20 [00:45<00:00,  2.28s/it]\n",
      "Processing Batch 7/10: 100%|██████████| 20/20 [00:46<00:00,  2.31s/it]\n",
      "Processing Batch 8/10: 100%|██████████| 20/20 [00:47<00:00,  2.35s/it]\n",
      "Processing Batch 9/10: 100%|██████████| 20/20 [00:45<00:00,  2.29s/it]\n",
      "Processing Batch 10/10: 100%|██████████| 6/6 [00:13<00:00,  2.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing complete!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "df = pd.read_csv(\"../evaluation/test_queries.csv\")\n",
    "\n",
    "if 'rag_v8a_output' not in df.columns:\n",
    "    df['rag_v8a_output'] = [''] * len(df)\n",
    "\n",
    "batch_size = 20\n",
    "num_batches = (len(df) + batch_size - 1) // batch_size\n",
    "\n",
    "for batch_num in range(num_batches):\n",
    "    start_idx = batch_num * batch_size\n",
    "    end_idx = min((batch_num + 1) * batch_size, len(df))\n",
    "    batch_indices = range(start_idx, end_idx)\n",
    "\n",
    "    batch_outputs = []\n",
    "\n",
    "    for idx in tqdm(batch_indices, desc=f\"Processing Batch {batch_num + 1}/{num_batches}\"):\n",
    "        # Check if the row is already processed (non-empty output)\n",
    "        if pd.notna(df.loc[idx, 'rag_v8a_output']) and df.loc[idx, 'rag_v8a_output'].strip():\n",
    "            batch_outputs.append(df.loc[idx, 'rag_v8a_output'])\n",
    "            continue\n",
    "\n",
    "        input_text = df.loc[idx, 'rag_v8_output']\n",
    "\n",
    "        try:\n",
    "            fixed_output = fix_syntax_chain.invoke(input_text)\n",
    "        except Exception as e:\n",
    "            fixed_output = \"ERROR\"\n",
    "\n",
    "        batch_outputs.append(fixed_output)\n",
    "\n",
    "    df.loc[start_idx:end_idx - 1, 'rag_v8a_output'] = batch_outputs\n",
    "    \n",
    "    # Save progress after each batch\n",
    "    df.to_csv(\"../evaluation/test_queries.csv\", index=False)\n",
    "\n",
    "print(\"Processing complete!\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
