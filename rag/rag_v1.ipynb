{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setting up Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "os.environ[\"LANGCHAIN_API_KEY\"] = os.getenv(\"LANGCHAIN_API_KEY\")\n",
    "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
    "os.environ[\"USER_AGENT\"] = \"myagent\"\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "os.environ[\"GROQ_API_KEY\"] = os.getenv(\"GROQ_API_KEY\")\n",
    "\n",
    "from langchain_groq import ChatGroq\n",
    "\n",
    "llm = ChatGroq(model=\"llama-3.1-8b-instant\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading Documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders.csv_loader import CSVLoader\n",
    "\n",
    "csv_loader = CSVLoader(file_path=\"../dataset/sample_promql_queries.csv\")\n",
    "\n",
    "csv_documents = csv_loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import bs4\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "\n",
    "urls = [\n",
    "    \"https://prometheus.io/docs/prometheus/latest/querying/basics/\",\n",
    "    \"https://prometheus.io/docs/prometheus/latest/querying/operators/\",\n",
    "    \"https://prometheus.io/docs/prometheus/latest/querying/functions/\",\n",
    "    \"https://prometheus.io/docs/prometheus/latest/querying/examples/\",\n",
    "    \"https://prometheus.io/docs/prometheus/latest/querying/api/\",\n",
    "    \"https://prometheus.io/docs/prometheus/latest/http_sd/\",\n",
    "    \"https://promlabs.com/promql-cheat-sheet/\",\n",
    "]\n",
    "\n",
    "bs4_strainer = bs4.SoupStrainer([\"h2\", \"h3\", \"p\", \"ul\", \"code\"])\n",
    "\n",
    "web_loader = WebBaseLoader(\n",
    "    web_paths=urls,\n",
    "    bs_kwargs={\"parse_only\": bs4_strainer}\n",
    ")\n",
    "\n",
    "web_documents = web_loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize metadata\n",
    "for doc in web_documents:\n",
    "    doc.metadata[\"row\"] = -1\n",
    "\n",
    "# Combine documents\n",
    "documents = csv_documents + web_documents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chunking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1000,\n",
    "    chunk_overlap=200,\n",
    "    add_start_index=True,\n",
    ")\n",
    "\n",
    "docs = text_splitter.split_documents(documents)\n",
    "\n",
    "# print(len(docs))\n",
    "# print(docs[1020].page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Embedding and Storing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_chroma import Chroma\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "\n",
    "hf_embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "\n",
    "vectorstore = Chroma.from_documents(documents=docs, embedding=hf_embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Context Retreival"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = vectorstore.as_retriever(search_kwargs={\"k\": 4})\n",
    "\n",
    "# retrieved_docs = retriever.invoke(\"Get the average CPU usage over the last 30 minutes.\")\n",
    "# print(retrieved_docs[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prompt Template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "template = \"\"\"\n",
    "You are an AI assistant that generates PromQL queries from natural language descriptions.\n",
    "You have access to the following context, which contains relevant information about metrics, labels, and PromQL syntax.\n",
    "\n",
    "### Context:\n",
    "{context}\n",
    "\n",
    "### Instructions:\n",
    "1. Analyze the natural language input to identify key metrics, time ranges, and functions.\n",
    "2. Use Prometheus functions and operators where appropriate.\n",
    "3. Ensure that the syntax is correct for PromQL and provides the required information.\n",
    "4. Keep the answer concise, focusing only on the final PromQL query.\n",
    "\n",
    "### Input:\n",
    "{query}\n",
    "\n",
    "### Example:\n",
    "**Input**: \"What is the average CPU usage over the past 5 minutes?\"\n",
    "\n",
    "**Output**: `avg(rate(cpu_usage[5m]))`\n",
    "\n",
    "### Output:\n",
    "\"\"\"\n",
    "\n",
    "custom_rag_prompt = PromptTemplate.from_template(template)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RAG Chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'rate(http_responses_error_total[15m])'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "def format_docs(docs):\n",
    "  return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "rag_chain = (\n",
    "    {\"context\": retriever | format_docs , \"query\": RunnablePassthrough()}\n",
    "    | custom_rag_prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "rag_chain.invoke(\"Show the error rate for HTTP responses in the last 15 minutes.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Output Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "promql_extraction_template = \"\"\"\n",
    "From the input provided, extract and return only the PromQL query. Do not include any additional text or formatting.\n",
    "\n",
    "input: {query}\n",
    "\"\"\"\n",
    "\n",
    "promql_extraction_prompt = PromptTemplate.from_template(promql_extraction_template)\n",
    "\n",
    "promql_extraction_chain = (\n",
    "    {\"query\": RunnablePassthrough()}\n",
    "    | promql_extraction_prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"../evaluation/test_queries.csv\")\n",
    "\n",
    "df['rag_v1_output'] = [''] * len(df)\n",
    "\n",
    "rag_v1_outputs = [];\n",
    "\n",
    "for query in df['nl_query']:\n",
    "    rag_v1_output = rag_chain.invoke(query)\n",
    "    final_query = promql_extraction_chain.invoke(rag_v1_output)\n",
    "    rag_v1_outputs.append(final_query)\n",
    "\n",
    "df['rag_v1_output'] = rag_v1_outputs\n",
    "\n",
    "df.to_csv('../evaluation/test_queries.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
