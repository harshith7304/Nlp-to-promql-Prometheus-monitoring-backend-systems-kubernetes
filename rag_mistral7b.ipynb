{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "AFhYDHOGcaOB",
    "outputId": "b56f0756-163f-44f8-f22d-cab6603bc378"
   },
   "outputs": [],
   "source": [
    "%pip install langchain_huggingface"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MyiaeB2QFCSi"
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "tBd8fqDqdNsr",
    "outputId": "8470aab7-0f77-403a-fcf2-0587876347d5"
   },
   "outputs": [],
   "source": [
    "%pip install langchain-chroma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "1QtWjCQ8eRP1",
    "outputId": "b96aaaea-5754-4a7f-8947-b5a10503943d"
   },
   "outputs": [],
   "source": [
    "%pip install InstructorEmbedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "nUWkVzZCb1Lf",
    "outputId": "9694739a-fe6b-4798-f851-f895abc13520"
   },
   "outputs": [],
   "source": [
    "\n",
    "%pip install langchain requests beautifulsoup4 openai nltk faiss-cpu sentence-transformers chromadb huggingface_hub langchain_community\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "uaTbTPhvgWZj",
    "outputId": "6237aaf4-a7ec-498d-af79-fd7919d64be7"
   },
   "outputs": [],
   "source": [
    "%pip install --upgrade langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1ii-6QvUlK_M",
    "outputId": "deb80ba9-eb4f-4e7a-b443-c9b943ceaf19"
   },
   "outputs": [],
   "source": [
    "%pip install pymupdf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "J1i8s9G6gAfM",
    "outputId": "7296c022-eef0-49e7-dc29-cab4d0f00455"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QnA4n1vSuhra",
    "outputId": "e133232e-eaea-413e-f652-6eb8ec9fe2a0"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from langchain_huggingface import HuggingFaceEndpoint\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.document_loaders import WebBaseLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.schema import Document\n",
    "import fitz  # PyMuPDF\n",
    "\n",
    "repo_id = \"mistralai/Mistral-7B-Instruct-v0.3\"\n",
    "llm = HuggingFaceEndpoint(repo_id=repo_id, model_kwargs={\"max_length\": 128})\n",
    "embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "\n",
    "def fetch_website_content(url):\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    text = ' '.join(p.get_text() for p in soup.find_all('p'))\n",
    "    return Document(page_content=text, metadata={\"source\": url})\n",
    "\n",
    "def fetch_pdf_content(file_path):\n",
    "    doc = fitz.open(file_path)\n",
    "    text = \"\"\n",
    "    for page in doc:\n",
    "        text += page.get_text()\n",
    "    return Document(page_content=text, metadata={\"source\": file_path})\n",
    "\n",
    "resources = [\n",
    "    \"https://prometheus.io/docs/prometheus/latest/querying/examples/\",\n",
    "    \"https://promlabs.com/promql-cheat-sheet/\",\n",
    "    \"https://prometheus.io/docs/prometheus/latest/querying/basics/\",\n",
    "    \"https://prometheus.io/docs/prometheus/latest/querying/operators/\",\n",
    "    \"https://prometheus.io/docs/prometheus/latest/querying/functions/\",\n",
    "    \"https://prometheus.io/docs/prometheus/latest/querying/api/\",\n",
    "    \"https://prometheus.io/docs/prometheus/latest/http_sd/\",\n",
    "    \"/content/drive/MyDrive/exp/Dataset.csv - Sheet1.csv\"\n",
    "]\n",
    "\n",
    "documents = []\n",
    "for resource in resources:\n",
    "    if resource.startswith(\"http\"):\n",
    "        documents.append(fetch_website_content(resource))\n",
    "    elif resource.endswith(\".pdf\"):\n",
    "        documents.append(fetch_pdf_content(resource))\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "splits = text_splitter.split_documents(documents)\n",
    "\n",
    "vectorstore = Chroma.from_documents(documents=splits, embedding=embeddings)\n",
    "\n",
    "class SimpleRetriever:\n",
    "    def __init__(self, vectorstore):\n",
    "        self.vectorstore = vectorstore\n",
    "\n",
    "    def get_relevant_documents(self, query):\n",
    "        return self.vectorstore.similarity_search(query)\n",
    "\n",
    "retriever = SimpleRetriever(vectorstore)\n",
    "\n",
    "prompt_template = \"\"\"\n",
    "Given the following context:\n",
    "{context}\n",
    "\n",
    "Answer the question:\n",
    "{question}\n",
    "\n",
    "Provide only the PromQL query enclosed in code blocks.\n",
    "\"\"\"\n",
    "\n",
    "def format_docs(docs):\n",
    "    return \" \".join([doc.page_content for doc in docs])\n",
    "\n",
    "class CustomStrOutputParser:\n",
    "    def __call__(self, output):\n",
    "        start_index = output.find('```')\n",
    "        end_index = output.rfind('```')\n",
    "        if start_index != -1 and end_index != -1 and start_index != end_index:\n",
    "            return output[start_index + 3:end_index].strip()\n",
    "        else:\n",
    "            return output.strip()\n",
    "\n",
    "def rag_chain(context, question):\n",
    "    formatted_context = format_docs(context)\n",
    "    prompt = prompt_template.format(context=formatted_context, question=question)\n",
    "    response = llm.invoke(prompt)  \n",
    "    return CustomStrOutputParser()(response)\n",
    "\n",
    "def invoke_without_history(query):\n",
    "    \n",
    "    relevant_docs = retriever.get_relevant_documents(query)\n",
    "    formatted_context = format_docs(relevant_docs)\n",
    "    prompt = prompt_template.format(context=formatted_context, question=query)\n",
    "    result = llm.invoke(prompt)\n",
    "    parsed_result = CustomStrOutputParser()(result)\n",
    "\n",
    "    return {\"answer\": parsed_result, \"sources\": relevant_docs}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uIdA2IMLo9rY"
   },
   "outputs": [],
   "source": [
    "def ask_question(query):\n",
    "    result = invoke_without_history(query)\n",
    "\n",
    "    if isinstance(result, dict) and \"answer\" in result and \"sources\" in result:\n",
    "        answer = result[\"answer\"]\n",
    "        relevant_docs = result[\"sources\"] \n",
    "    else:\n",
    "        answer = result \n",
    "        relevant_docs = []\n",
    "\n",
    "    sources = set(doc.metadata[\"source\"] for doc in relevant_docs)\n",
    "\n",
    "    print(f\"Query: {query}\")\n",
    "    print(f\"Answer: {answer}\")\n",
    "    print(f\"Sources: {sources}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IA7vRxTqpCSE",
    "outputId": "dd71d376-1a46-432c-ded0-d2d186f4ab27"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: Give me the disk space usage percentage for the past 12 hours.\n",
      "Answer: 100 - (1 - (sum(rate(node_filesystem_usage_bytes{mountpoint=\"/\", fstype=\"ext4\"}[12h]) / sum(node_filesystem_usage_bytes{mountpoint=\"/\", fstype=\"ext4\"}))) * 100)\n",
      "Sources: {'https://promlabs.com/promql-cheat-sheet/'}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ask_question(\"Give me the disk space usage percentage for the past 12 hours.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cDVv_mJ_pYuj",
    "outputId": "ecea4055-81d0-437f-9496-2ee02c87af23"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: whats the average response time of our api in last 5 minutes?\n",
      "Answer: sum(rate(http_requests_total{job=~\"job$\\\\[a-z]*server\\\\b\"}[5m])) / sum(rate(http_requests_total{job=~\"job$\\\\[a-z]*server\\\\b\"}[5m]))\n",
      "Sources: {'https://prometheus.io/docs/prometheus/latest/querying/examples/'}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ask_question(\"whats the average response time of our api in last 5 minutes?\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
